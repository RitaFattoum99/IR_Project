{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85d168e",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891e56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words;\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mysql.connector\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize as tokenizer\n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_maksura_ar, normalize_teh_marbuta_ar, normalize_alef_ar\n",
    "from camel_tools.utils.dediac import dediac_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d823d",
   "metadata": {},
   "source": [
    "Loading Corpus, Queries, Qrels files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e8c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading en_corpus\n",
    "en_corpus = pd.read_csv('C:/Users/Ahmed/Desktop/data/en_corpus.csv')\n",
    "\n",
    "#loading en_quereies\n",
    "en_queries = pd.read_json(\"C:/Users/Ahmed/Desktop/data/en_queries.jsonl\", lines=True)\n",
    "\n",
    "#loading en_qrels\n",
    "en_qrels = pd.read_csv(\"C:/Users/Ahmed/Desktop/data/en_qrels.tsv\", sep=\"\\t\")\n",
    "\n",
    "#loading ar_corpus\n",
    "ar_corpus = pd.read_csv('C:/Users/Ahmed/Desktop/data/ar_corpus.csv')\n",
    "\n",
    "#loading ar_quereies\n",
    "ar_queries = pd.read_csv(\"C:/Users/Ahmed/Desktop/data/ar_queries.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# #loading ar_qrels file\n",
    "ar_qrels = pd.read_csv(\"C:/Users/Ahmed/Desktop/data/ar_qrels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff04dbb",
   "metadata": {},
   "source": [
    "English Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72396536-331f-4d3b-9a6f-c67e8f0cdec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#English Text Preprocessing\n",
    "\n",
    "stopWords = stopwords.words('English') + [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would','arent', 'couldnt', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'nt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve''arent', 'couldnt', 'didnt', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'nt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve']\n",
    "additonal_words = ['cigarettes', 'vaping', 'vape', 'privatize', 'palestinian', 'israeli']\n",
    "words = set(words.words())\n",
    "words.update(additonal_words)\n",
    "string.punctuation = \"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "#\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~≤…•¾–—‘’“”≡\"\n",
    "lemmatizerr = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag_parameter):\n",
    "    tag = tag_parameter[0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def list_to_string(list):\n",
    "    return ' '.join(map(str, list))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def normalization(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_links(text):\n",
    "    return re.sub(r'\\b(?:https?://|www\\d{0,3}\\.)\\S+\\b', '', text)\n",
    "\n",
    "def stop_words_remove(text):\n",
    "    result = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in stopWords:\n",
    "            result.append(word)\n",
    "    return list_to_string(result)\n",
    "\n",
    "def verb_adj_lemma(text):\n",
    "    words1 = word_tokenize(text)\n",
    "    text1 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='v') for word in words1])\n",
    "    words2 = word_tokenize(text1)\n",
    "    text2 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='a') for word in words2])\n",
    "    text_pos = pos_tag(word_tokenize(text2)) \n",
    "    answer = ' '.join([WordNetLemmatizer().lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in text_pos])\n",
    "    return answer\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text_pos = pos_tag(word_tokenize(text))\n",
    "    lemmatized = [lemmatizerr.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in text_pos]\n",
    "    return list_to_string(lemmatized)\n",
    "\n",
    "def stemmer(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return list_to_string(stemmed_words)\n",
    "\n",
    "def punctuation_remove(text):\n",
    "    return text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "def remove_meaningless_word(text):\n",
    "    return \" \".join(w for w in word_tokenize(text) if w.lower() in words or not w.isalpha())\n",
    "\n",
    "def remove_spaces(text):\n",
    "    return \" \".join(text.split())\n",
    "    \n",
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "def remove_short_words(text):\n",
    "    return re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "\n",
    "def preProcessor(text):\n",
    "    text1 = normalization(text)\n",
    "    text2 = remove_links(text1)\n",
    "    text3 = remove_numbers(text2)\n",
    "    text4 = punctuation_remove(text3)\n",
    "    text5 = remove_non_alphanumeric(text4)\n",
    "    text6 = remove_spaces(text5)\n",
    "    text7 = verb_adj_lemma(text6)\n",
    "    text8 = stop_words_remove(text7)\n",
    "    text9 = remove_short_words(text8)\n",
    "    text10 = remove_meaningless_word(text9)\n",
    "    return text10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d1acc8",
   "metadata": {},
   "source": [
    "Arabic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb766677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arabic Text Preprocessing\n",
    "\n",
    "# Remove non-Arabic characters\n",
    "def remove_non_arabic(text):\n",
    "    arabic_text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', str(text))\n",
    "    return arabic_text\n",
    "    \n",
    "# Remove Arabic numbers\n",
    "def remove_arabic_numbers(text):\n",
    "    return re.sub(r'[٠-٩]', '', text)\n",
    "\n",
    "# Punctuation removal\n",
    "arabic_punctuation = \"\"\"/ːː،؛؟.٪؛،:«»::–()[]{}<>+=-%*/&:|~\\\\''``\"\"\"\n",
    "translator = str.maketrans('', '', arabic_punctuation)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.translate(translator)\n",
    "    return text\n",
    "\n",
    "# Initialize disambiguators\n",
    "mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "msa_atb_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme='atbtok')\n",
    "\n",
    "# Regex pattern to split by _ and +\n",
    "pattern = re.compile(r'[^_+]+')\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords_arabic(text):\n",
    "    if pd.isna(text):\n",
    "        return text \n",
    "    custom_stopwords = ['لل', 'أليس', '', 'ك', 'س', 'ف', 'ب', 'أو', 'و', 'ما', 'لو', 'ال', 'لا', 'ّ', 'ٌ', ' ', 'ء', 'ئ', '‘', '؛', 'أ', 'إ', ',', '’', 'آ', '~', 'ًٍَُِْ', ' ', '  ', 'ؤ', ' ', ' ', ' ', ' ', 'وهي', 'او', 'و', 'بهذا', 'هذا',  'وايضا', 'ايضا', 'ومع', 'مع', 'ما', 'وما', 'والتي', 'اليها', 'الي','علي','على', 'كان', 'ان', 'في', 'التي', 'اذا','معظم','هي',  'متي','متى','الى','م','سم','لهذا','ال','حوالي', 'لأ', 'NOAN']\n",
    "    stop_words = set(stopwords.words('arabic')) | set(custom_stopwords)\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "    \n",
    "# Normalization\n",
    "def normalize_arabic_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text \n",
    "    text = normalize_unicode(text)\n",
    "    text = normalize_alef_maksura_ar(text)\n",
    "    text = normalize_teh_marbuta_ar(text)\n",
    "    text = normalize_alef_ar(text)\n",
    "    text = dediac_ar(text)\n",
    "    return text\n",
    "\n",
    "# Stemming\n",
    "st = ISRIStemmer()\n",
    "def stem_arabic_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [st.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "# Explicitly remove NOAN\n",
    "def remove_specific_word(text, word):\n",
    "    return ' '.join([t for t in text.split() if t != word])\n",
    "\n",
    "# All text processors\n",
    "def text_proccessers(text):\n",
    "    text = remove_non_arabic(text)\n",
    "    text = tokenizer(text)\n",
    "    text = msa_atb_tokenizer.tokenize(text)\n",
    "    text = pattern.findall(' '.join(text))\n",
    "    text = ' '.join(text)\n",
    "    text = remove_stopwords_arabic(text)\n",
    "    text = remove_arabic_numbers(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = normalize_arabic_text(text)\n",
    "    text = stem_arabic_text(text)\n",
    "    text = remove_specific_word(text, \"NOAN\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420b904",
   "metadata": {},
   "source": [
    "Building Index usin tfidfvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning Model & indexing\n",
    "\n",
    "def save_terms(path , vectorizer):\n",
    "    with open(path, 'w') as outfile:\n",
    "        outfile.writelines(f\"{i}\\n\" for i in vectorizer.get_feature_names_out())\n",
    "    return \"Saving Terms Done\"\n",
    "\n",
    "def save_model(path, vectorizer):\n",
    "    with open(path, 'wb') as outp:\n",
    "        pickle.dump(vectorizer, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    return \"Saving model Done\"\n",
    "\n",
    "def save_index(path, index):\n",
    "    sparse.save_npz(path, index)\n",
    "    return \"Saving index Done\"\n",
    "\n",
    "def traning_model(lang, docs: list, preprocessor, tokenizer):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer, preprocessor=preprocessor)\n",
    "    index = vectorizer.fit_transform(docs)\n",
    "    save_terms(f'C:/Users/Ahmed/Desktop/ir_output/{lang}_terms.txt', vectorizer)\n",
    "    save_model(f'C:/Users/Ahmed/Desktop/ir_output/{lang}_model.pkl', vectorizer)\n",
    "    save_index(f'C:/Users/Ahmed/Desktop/ir_output/{lang}_index.npz', index)\n",
    "    return \"Traning Model & indexing doen!\"\n",
    "\n",
    "traning_model(\"en\", en_corpus['title'], preProcessor, tokenizer)\n",
    "traning_model(\"ar\", ar_corpus['text'], text_proccessers, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a3208",
   "metadata": {},
   "source": [
    "Matching, Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7f1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matching (Cosine Similarity)\n",
    "\n",
    "#loading model object\n",
    "# with open('C:/Users/Ahmed/Desktop/IR_output_en_prev/tfidfvectorizer_object.pkl', 'rb') as inp:\n",
    "#     vectorizer = pickle.load(inp)\n",
    "\n",
    "# #Loading index\n",
    "# docs_vector = sparse.load_npz(\"C:/Users/Ahmed/Desktop/IR_output_en_prev/csr_martix.npz\")\n",
    "\n",
    "# #Cosine Similarity method 1, each query sorted and stored in csv file\n",
    "# test = pd.DataFrame(0, index=[\"cosine\"], columns=corpus[\"_id\"], dtype=np.float64)\n",
    "# for index, row in queries.iterrows():\n",
    "#     query_vector = vectorizer.transform([row['text']])\n",
    "#     result = cosine_similarity(docs_vector, query_vector).flatten()\n",
    "#     test.loc['cosine'] = result\n",
    "#     sorted_test = test.sort_values(by=\"cosine\", axis=1, ascending=False)\n",
    "#     sorted_test.to_csv(f\"C:/Users/Ahmed/Desktop/cosine_similarity/{index}.csv\", index=False)\n",
    "# print(\"done cosine similarity\")\n",
    "\n",
    "# #calculate MAP\n",
    "# map_result = np.full((49, 10), False)\n",
    "# for index, row in queries.iterrows():\n",
    "#     cos_similarity = pd.read_csv(f\"C:/Users/Ahmed/Desktop/cosine_similarity/{index}.csv\")\n",
    "#     first_ten_values = cos_similarity.iloc[0, :10]\n",
    "#     for k, (doc, cos) in enumerate(first_ten_values.items()):\n",
    "#         for i, value in qrels.iterrows():\n",
    "#             if(index == value[\"query-id\"]-1):\n",
    "#                 if(doc == value[\"corpus-id\"]):\n",
    "#                     map_result[index][k] = True\n",
    "#             else: continue\n",
    "# np.savetxt('C:/Users/Ahmed/Desktop/map.txt', map_result, fmt='%d')\n",
    "# print(\"done calculating map\")\n",
    "\n",
    "\n",
    "#Matching Service\n",
    "def load_model(path):\n",
    "    with open(path, 'rb') as inp:\n",
    "        return pickle.load(inp)\n",
    "    \n",
    "def load_index(path):\n",
    "    return sparse.load_npz(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abd8d0e",
   "metadata": {},
   "source": [
    "Calculate Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claculate_map(self):\n",
    "    actual = self.revelance_matrix()\n",
    "    Q = len(actual)\n",
    "    predicted = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    k = 10\n",
    "    ap = []\n",
    "    ap_num = 0    \n",
    "    for x in range(k):        \n",
    "        act_set = set(actual[self.q])        \n",
    "        pred_set = set(predicted[:x+1])\n",
    "        precision_at_k = len(act_set & pred_set) / (x+1)\n",
    "        if predicted[x] in actual[self.q]:\n",
    "            rel_k = 1        \n",
    "        else:\n",
    "            rel_k = 0    \n",
    "        ap.append(self.ap_q)\n",
    "    return sum(ap) / Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee131b4",
   "metadata": {},
   "source": [
    "Search Api Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "099f01f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('الشمالية (توضيح) الشمالية ( توضيح ) - الشمالية ( ولاية ) - الشمالية ( عين العرب ) - الشمالية ( مدينة بحرينية ) - الشمالية ( محافظة ) - الحدود الشمالية ( منطقة )',)],\n",
       " [('أسكاون (توضيح) أسكاون ( توضيح ) قد يقصد من « أسكاون » : - أسكاون - أسكاون ( أداي ) - أسكاون ( إد نوح )',)],\n",
       " [('بشملة  البَشْمَلَة أو أو هو جنس نباتي يتبع الفصيلة الوردية من رتبة الورديات . والبشملة شجر دائم الخضرة، تعطي الشجرة ثمار صغيرة بيضاوية الشكل صفراء اللون عند نضجها . ويعرف في بعض اللهجات المحلية باسم الأكي دنيا و أسكيدنيا و ناسبولي، وتسمى أيضا بُوصَاع في تونس و عرنوط في بعض اللهجات المحلية الأخرى، و تعرف بالمغرب باسم مزاح . أصل الاسم أكي دنيا هو تركي ويعني الدنيا الجديدة . يوجد عدة أنواع من جنس البشملة : - بشملة يابانية - بشملة منحنية - بشملة كافاليرية - بشملة عطرية - بشملة هنرية - بشملة ماليبونية - بشملة بيضاوية - بشملة سنديانية - بشملة سالوينية - بشملة سيغوينية - بشملة مسننة - بشملة تنغشونغية - بشملة بنغالية - بشملة غامضة - بشملة إهليلجية - بشملة جرداء - بشملة هوكرية - بشملة عريضة الأوراق - بشملة طويلة الأوراق - بشملة كبيرة الثمار - بشملة مرغوية - بشملة معنقة - بشملة مفلطحة الأوراق - بشملة بوالنية - بشملة أذينية - بشملة صبغية - بشملة واردية - بشملة مستطيلة',)],\n",
       " [('كشمولة (توضيح) كشمولة ( توضيح ) كشمولة قد تشير إلى : - يوسف كشمولة - دريد كشمولة ( و . 1943 ) - أسامة كشمولة ( ت . 1425 هـ / 2004 م ) - [ قاسم خليل أبراهيم كشمولة ] [ و.1996 ]',)],\n",
       " [('علم ألاسكا  اعتمد هذا العلم رسميا في سنة 1927 عندما كانت ألاسكا إقليما وليست ولاية تابعة للاتحاد الأمريكي ( انضمت ألاسكا إلى الاتحاد الأمريكي في سنة 1959 اي بعد 32 عاما من اقرار العلم ) .',)],\n",
       " [('الولايات الأمريكية المتجاورة  الولايات الأمريكية الثمانية والأربعين المتجاورة هو مصطلح يقصد به الولايات الأمريكية التي تقع شمال الحدود المكسيكية وجنوب الحدود الكندية؛ بصيغة أخرى هي جميع الولايات الأمريكية التي تجاور ولايات أخرى عن طريق البر، ويشمل هذا التصنيف جميع الولايات المتحدة الأمريكية ما عدا ولايتي هاواي وألاسكا .',)],\n",
       " [('إسكان (توضيح) إسكان ( توضيح ) قد يقصد من « إسكان » : - إسكان ( آيت واوكرضة ) - إسكان ( أولوز ) - إسكان ( عفرين )',)],\n",
       " [('أسكا (توضيح) أسكا ( توضيح ) قد يقصد من « أسكا » : - أسكا ( آيت بوبكر ) - أسكا ( آيت تونارت ) - أسكا ( آيت ضوشن ) - أسكا ( آيت موسى ) - أسكا ( آيت ودجاس ) - أسكا ( تلارواق ) - أسكا ( ربع نتزومت ) - أسكا ( كنطولة ) - أسكا ( ويجان )',)],\n",
       " [('تيار آلاسكا  تيار آلاسكا هو تيار مائي حار يعد استمراراً شمالياً لجزء من مياه تيار الهادئ الشمالي المنحرفة شمالاً لدى اصطدامها بالساحل الغربي للقارة الأمريكية الشمالية , ويسير هذا التيار بمحاذاة ساحل آلاسكا الغربي متوجهاً نحو الشمال . المصدر المعجم الجغرافي المناخي',)],\n",
       " [('مقاطعة لامور (داكوتا الشمالية) مقاطعة لامور ( داكوتا الشمالية ) لامور هي إحدى مقاطعات ولاية داكوتا الشمالية في الولايات المتحدة الأمريكية .',)],\n",
       " [('اسكا (توضيح) اسكا ( توضيح ) قد يقصد من « اسكا » : - اسكا ( آيت احسي لحسن ) - اسكا ( آيت ملال ) - اسكا ( آيت واحمدن )',)],\n",
       " [('شميلان (توضيح) شميلان ( توضيح ) قد يقصد من « شميلان » : - شميلان ( أبو الفارس ) - شميلان ( العدين ) - شميلان ( دهسرد )',)],\n",
       " [('سعادي أبيض وأسود  السعادي الأبيض والأسود نوع نباتي يتبع جنس السعادي من الفصيلة السعدية . موطنه الأصلي غرب أمريكا الشمالية وأمريكا الشمالية حيث ينتشر في معظم مناطق غرب الولايات المتحدة الأمريكية وكندا .',)],\n",
       " [('المنطقة الشمالية (الكاميرون) المنطقة الشمالية ( الكاميرون ) المنطقة الشمالية ( المحافظة الشمالية حتى عام 2008 ) ، تُشكّل 66,090 كم من مساحة النصف الشمالي من جمهورية الكاميرون . تشمل الأراضي المجاورة لمنطقة أقصى الشمال إلى الشمال، ومنطقة أداماوا إلى الجنوب، ونيجيريا إلى الغرب، وتشاد إلى الشرق، وجمهورية أفريقيا الوسطى إلى الجنوب الشرقي . مدينة غاروا هي العاصمة السياسية والصناعية .',)],\n",
       " [('آيت فاسكا (آيت فاسكا) آيت فاسكا ( آيت فاسكا ) آيت فاسكا هي إحدى مشيخات المملكة المغربية، تتبع جغرافيا لإقليم الحوز وإداريًا لآيت فاسكا . يقدر عدد سكانها بـ 9116 نسمة حسب الإحصاء الرسمي للسكان والسكنى لسنة 2004 . انظر أيضا . - قائمة مدن المغرب',)],\n",
       " [('مطبخ أمريكا الشمالية  مطبخ أمريكا الشمالية مصطلح يشمل المأكولات الشعبية المنتشرة في دول أمريكا الشمالية والتي تشمل الولايات المتحدة الأميركية وكندا والمكسيك . ويمكن للمصطلح أن يشتمل أيضاً على مطبخ أمريكا الوسطى وجزر الكاريبي . والمعروف أن أطعمة هذه المناطق تأثرت بالعديد من المطابخ العالمية وبخاصة الأوروبية والآسيوية وطرق طهي شعوب الأميركيين الأصليين .',)],\n",
       " [('المنطقة الشمالية (البحرين) المنطقة الشمالية ( البحرين ) كانت بلدية المنطقة الشمالية في الجزء الشمالي الغربي من مملكة البحرين . تم توزيع أراضيها على محافظة الشمالية .',)],\n",
       " [('أندرسون (ألاسكا) أندرسون ( ألاسكا ) أندرسون هي مدينة تقع في ولاية ألاسكا في الولايات المتحدة . تبلغ مساحة هذه المدينة 122.4 ( كم² ) ، وترتفع عن سطح البحر 157 م، بلغ عدد سكانها نسمة في عام 2010 حسب إحصاء مكتب تعداد الولايات المتحدة . وصلات خارجية . - city website - community information',)],\n",
       " [('ألاسكا  ألاسكا ( بالإنجليزية : alaska ) هي إحدى ولايات إقليم المحيط الهادي والذي يضم بالإضافة لألاسكا أربع ولايات هي ولاية واشنطن، وولاية أوريجون، وولاية كاليفورنيا، وهاواي . وتعد ألاسكا أكبر ولاية في الولايات المتحدة الأمريكية وتوازي مساحتها خُمس بقية الولايات، واكثر من ضعف مساحة تكساس بقليل . حيث تبلغ مساحتها 1,518,776 كيلومتر مربع لكن حجمها الكبير يقابله تعداد سكاني ضئيل نسبيا مقارنة بالولايات الأخرى ففي تعداد السكان لعام 1990 كانت ألاسكا في المرتبة 49 من حيث عدد السكان في الولايات الأمريكية، وفقط ولاية وايومنغ كانت أقل منها سكانا، في إحصاء عام 1980 كان عدد السكان 400481 وفي إحصاء عام 2000 بلغ عدد سكان الولاية 626,932 نسمة . تتميز ألاسكا أنها ولاية منفصلة عن بقية أراضي الولايات المتحدة، فهي تقع شمال غرب كندا، ويفصلها عن ولاية واشنطن 500 ميل ( 800 كيلومتر ) من الأراضي الكندية . اشترتها الولايات المتحدة من روسيا القيصرية سنة ( 1284 هـ -1867 م ) بسبعة ملايين ومائتي ألف دولار وتم اعتمادها رسميا كولاية في 3 يناير 1959 حيث كان ترتيبها 48 في عدد الولايات آنذاك، لذلك غالبا ما يسمي الألاسكيون باقي الولايات بال48 السفلى . يعتقد أن كلمة ألاسكا نحتت من كلمة إلياسكا alyeska، بمعنى الأرض الأكبر أو الأرض الأم في لغة شعب الألويت الإسكيمو .',)],\n",
       " [('آيت فاسكا (توضيح) آيت فاسكا ( توضيح ) قد يقصد من « آيت فاسكا » : - آيت فاسكا - آيت فاسكا ( آيت فاسكا ) - آيت فاسكا ( بن صميم ) - آيت فاسكا ( كرول )',)],\n",
       " [('سانفورد (كارولاينا الشمالية) سانفورد ( كارولاينا الشمالية ) سانفورد هي مدينة أمريكية ومركز مقاطعة لي في ولاية كارولاينا الشمالية في الولايات المتحدة الأمريكية .',)],\n",
       " [('قائمة مدن ألاسكا  هذه قائمة مدن ألاسكا في الولايات المتحدة مرتبة حسب تعداد السكان في 2010 .',)],\n",
       " [('أسكا أوبلاغ (أسكا) أسكا أوبلاغ ( أسكا ) أسكا أوبلاغ هو دُوَّار يقع بجماعة ويجان، إقليم تيزنيت، جهة سوس ماسة في المملكة المغربية . ينتمي الدوّار لمشيخة أسكا التي تضم 22 دوار . يقدر عدد سكانه بـ 343 نسمة حسب الإحصاء الرسمي للسكان والسكنى لسنة 2004 . روابط خارجية . - البوابة الوطنية للجماعات الترابية - المندوبية السامية للتخطيط',)],\n",
       " [('حاسي القارة  إحدى بلديات ولاية غرداية ، وتقع جغرافيا في أقصى الجنوب الغربي للولاية .',)],\n",
       " [('شمال  الشمال هو أحد الإتجاهات الأربعة، ويقع أعلي خطّ الاستواء، في الجهة المعاكسة لاتجاه الجنوب . أقصى الشمال هو القطب الشمالي .+ يوجد شمالين : الأول شمال مغناطيسي ( للبوصلة ) أما الثاني فهو الشمال الجغرافي وهو الشمال الحقيقي . المسافة الحقيقية بينهما عند القطب الشمالى هي 1900 كم . يتغير الفرق بينهما نتيجة لتغير في مغناطيسية الأرض حيث يتحرك الشمال المغناطيسى في قوس حول الشمال الحقيقى يغير اتجاهه كل خمسة وعشرون الف سنة حاليا كل سنة يزداد الفرق ب 0.8 درجة .',)],\n",
       " [('أداك (ألاسكا) أداك ( ألاسكا ) أداك هي مدينة تقع في ولاية ألاسكا في الولايات المتحدة . تبلغ مساحة هذه المدينة 329.7 ( كم² ) ، وترتفع عن سطح البحر 50 م، بلغ عدد سكانها نسمة في عام 2010 حسب إحصاء مكتب تعداد الولايات المتحدة . وصلات خارجية . - city website - community information',)],\n",
       " [('قرية إسكان  قرية إسكان أو كما تسمى رسميا مجمع قرية إسكان للقوات الجوية هي قاعدة عسكرية أمريكية تقع على بعد 20 كيلومترا جنوب شرق الرياض في المملكة العربية السعودية . تُعد القاعدة مقرا ، و . بُنيت قرية إسكان سنة 1983 خلال عهد الملك فهد بن عبد العزيز آل سعود لإيواء عدة قبائل بدوية . رفضت القبائل البدوية السكن في المجمع وبقيت قرية إسكان غير مأهولة حتى أغسطس 1990 . في ذلك الوقت سمحت الحكومة السعودية بإستخدام قرية إسكان للجنود الأمريكيين أثناء حرب الخليج الثانية .',)],\n",
       " [(\"ولاية أمريكية  الولاية الأمريكية ( بالإنجليزية : u.s. state ) هي واحدة من الكيانات الجزئية الخمسين التي تقع بداخل الولايات المتحدة الأمريكية، على الرغم من استخدام أربع ولايات أمريكية اسم `` كومنولث '' كلقب رسمي . والولاية هي التقسيم الإداري الأولي في الولايات المتحدة وبلدان أخرى مثل أستراليا . تقسم الولاية إلى محافظات . بالإضافة إلى الولايات الخمسين، توجد مناطق أخرى تتبع الولايات المتحدة : واشنطن دي سي، بورتو ريكو، ساموا الأمريكية، غوام، جزر ماريانا الشمالية، الجزر العذراء الأمريكية . تتصل جميع الولايات ببعضها البعض، ماعدا ولايتي ألاسكا وهاواي .\",)],\n",
       " [('سيتكا (ألاسكا) سيتكا ( ألاسكا ) سيتكا هي مدينة تقع في ولاية ألاسكا في الولايات المتحدة . تبلغ مساحة هذه المدينة 12461.8 ( كم² ) ، وترتفع عن سطح البحر 8 م، بلغ عدد سكانها 8952 نسمة في عام 2010 حسب إحصاء مكتب تعداد الولايات المتحدة . وصلات خارجية . - www.cityofsitka.com - community information',)],\n",
       " [('إسكي شهر (محافظة) إسكي شهر ( محافظة ) محافظة إسكي شهر هي إحدى محافظات تركيا، وتقع في شمال غرب تركيا وعاصمتها مدينة إسكي شهر . تبلغ مساحتها 13,904 كم ويبلغ عدد سكانها 706,009 نسمة كما يبلغ معدل الكثافة السكانية بها 50 نسمة/كم .',)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"\",\n",
    "  database=\"IR\"\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "en_sql = \"SELECT content FROM corpus WHERE doc_id = %s\"\n",
    "ar_sql = \"SELECT content FROM ar_corpus WHERE doc_id = %s\"\n",
    "\n",
    "# # #query api function without database\n",
    "# # api_result = []\n",
    "# # en_query_vector = vectorizer.transform([\"Should teachers get tenure?\"])\n",
    "# # result = cosine_similarity(docs_vector, en_query_vector).flatten()\n",
    "# # en_test.loc['cosine'] = result\n",
    "# # en_test.sort_values(by=\"cosine\", axis=1, ascending=False ,inplace=True)\n",
    "# # ans = en_test.iloc[0, :30]\n",
    "# # for key, value in ans.items():\n",
    "# #     if(value < 0.6):\n",
    "# #         break\n",
    "# #     for index, row in corpus.iterrows():\n",
    "# #         if(row[\"_id\"] == key):\n",
    "# #             api_result.append(row[\"title\"])\n",
    "# # print(len(api_result))\n",
    "# # api_result\n",
    "\n",
    "\n",
    "# #query api function with database\n",
    "\n",
    "def search_api(query, language):\n",
    "  api_result = []\n",
    "  if(language == 'en'):\n",
    "    test = pd.DataFrame(0, index=[\"cosine\"], columns=en_corpus[\"_id\"], dtype=np.float64)\n",
    "    vectorizer = load_model('C:/Users/Ahmed/Desktop/ir_output_last/en_model.pkl')\n",
    "    index = load_index('C:/Users/Ahmed/Desktop/ir_output_last/en_index.npz')\n",
    "  else:\n",
    "    test = pd.DataFrame(0, index=[\"cosine\"], columns=ar_corpus[\"doc_id\"], dtype=np.float64)\n",
    "    vectorizer = load_model('C:/Users/Ahmed/Desktop/ir_output_last/ar_model.pkl')\n",
    "    index = load_index('C:/Users/Ahmed/Desktop/ir_output_last/ar_index.npz')\n",
    "\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    result = cosine_similarity(index, query_vector).flatten()\n",
    "    test.loc['cosine'] = result\n",
    "    test.sort_values(by=\"cosine\", axis=1, ascending=False ,inplace=True)\n",
    "    ans = test.iloc[0, :30]\n",
    "\n",
    "  if(language == 'en'):\n",
    "    for key, value in ans.items():\n",
    "      if(value < 0.6):\n",
    "        break\n",
    "      adr = (key, )\n",
    "      mycursor.execute(en_sql, adr)\n",
    "      myresult = mycursor.fetchall()\n",
    "      api_result.append(myresult)\n",
    "  else:\n",
    "    for key, value in ans.items():\n",
    "      if(value < 0.3):\n",
    "        break\n",
    "      adr = (key, )\n",
    "      mycursor.execute(ar_sql, adr)\n",
    "      myresult = mycursor.fetchall()\n",
    "      api_result.append(myresult)\n",
    "  return api_result\n",
    "\n",
    "search_api(\"624 ألاسكا () هي ولاية أمريكية تقع في أقصى الشمال الغربي لأمريكا الشمالية.\", 'ar')\n",
    "\n",
    "\n",
    "# api_result = []\n",
    "\n",
    "# en_test = pd.DataFrame(0, index=[\"cosine\"], columns=en_corpus[\"_id\"], dtype=np.float64)\n",
    "# en_vectorizer = load_model('C:/Users/Ahmed/Desktop/ir_output_last/en_model.pkl')\n",
    "# en_index = load_index('C:/Users/Ahmed/Desktop/ir_output_last/en_index.npz')\n",
    "\n",
    "# en_query_vector = en_vectorizer.transform([\"Should teachers get tenure?\"])\n",
    "# en_result = cosine_similarity(en_index, en_query_vector).flatten()\n",
    "# en_test.loc['cosine'] = en_result\n",
    "# en_test.sort_values(by=\"cosine\", axis=1, ascending=False ,inplace=True)\n",
    "# ans = en_test.iloc[0, :30]\n",
    "\n",
    "# ar_test = pd.DataFrame(0, index=[\"cosine\"], columns=ar_corpus[\"doc_id\"], dtype=np.float64)\n",
    "# ar_vectorizer = load_model('C:/Users/Ahmed/Desktop/ir_output_last/ar_model.pkl')\n",
    "# ar_index = load_index('C:/Users/Ahmed/Desktop/ir_output_last/ar_index.npz')\n",
    "\n",
    "\n",
    "# ar_query_vector = ar_vectorizer.transform([\"624 ألاسكا () هي ولاية أمريكية تقع في أقصى الشمال الغربي لأمريكا الشمالية.\"])\n",
    "# ar_result = cosine_similarity(ar_index, ar_query_vector).flatten()\n",
    "# ar_test.loc['cosine'] = ar_test\n",
    "# ar_test.sort_values(by=\"cosine\", axis=1, ascending=False ,inplace=True)\n",
    "# ans = ar_test.iloc[0, :30]\n",
    "\n",
    "# #Retreive Values\n",
    "# for key, value in ans.items():\n",
    "#   if(value < 0.6):\n",
    "#     break\n",
    "#   adr = (key, )\n",
    "#   mycursor.execute(sql, adr)\n",
    "#   myresult = mycursor.fetchall()\n",
    "#   api_result.append(myresult)\n",
    "\n",
    "\n",
    "# print(len(api_result))\n",
    "# api_result\n",
    "\n",
    "\n",
    "# query_ids = []\n",
    "# for index, row in ar_queries.iterrows():\n",
    "#     id = \"\"\n",
    "#     for c in row[\"query\"]:\n",
    "#         if(not c.isdigit()):\n",
    "#             query_ids.append(id)\n",
    "#             break\n",
    "#         id+=c\n",
    "\n",
    "# print(len(query_ids))\n",
    "\n",
    "# print(ar_queries.iloc[5000])\n",
    "# print(query_ids[5000])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
